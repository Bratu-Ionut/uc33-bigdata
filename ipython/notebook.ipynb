{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84a5a612-64ce-4e43-b5fa-90a674bbe5d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 348, 900, 238, 291]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark  \n",
    "\n",
    "sc = pyspark.SparkContext('local[*]') \n",
    "\n",
    "# do something to prove it works \n",
    "\n",
    "rdd = sc.parallelize(range(1000)) \n",
    "\n",
    "rdd.takeSample(False, 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8786b25d-1d8b-4361-8e12-a362ba608e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "account.csv  country_abbreviation.csv  transactions.csv  Untitled.ipynb  work\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f40e1b22-0baf-4cdf-8446-6cff9b5f408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSV File Count Example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8f22abf-58dd-46e5-bce4-00b7c83ce633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The account.csv file has 5 rows.\n",
      "The transactions.csv file has 10 rows.\n",
      "The country_abbreviation.csv file has 5 rows.\n"
     ]
    }
   ],
   "source": [
    "# Read account.csv file\n",
    "account_df = spark.read.csv(\"account.csv\", header=True)\n",
    "account_count = account_df.count()\n",
    "print(f\"The account.csv file has {account_count} rows.\")\n",
    "\n",
    "# Read transactions.csv file\n",
    "transactions_df = spark.read.csv(\"transactions.csv\", header=True)\n",
    "transactions_count = transactions_df.count()\n",
    "print(f\"The transactions.csv file has {transactions_count} rows.\")\n",
    "\n",
    "# Read country_abbreviation.csv file\n",
    "country_abbreviation_df = spark.read.csv(\"country_abbreviation.csv\", header=True)\n",
    "country_abbreviation_count = country_abbreviation_df.count()\n",
    "print(f\"The country_abbreviation.csv file has {country_abbreviation_count} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad2353a0-5ec0-4ad5-942e-7b9fcdf4d020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "account_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8de3f430-abc5-4487-b0ad-14092bc3de9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|account_type|account_type_count|\n",
      "+------------+------------------+\n",
      "|    checking|                 2|\n",
      "|     savings|                 3|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Spark SQL 2\n",
    "transactions_df = spark.read.csv(\"transactions.csv\", header=True)\n",
    "\n",
    "# Register the DataFrame as a SQL temporary table\n",
    "transactions_df.createOrReplaceTempView(\"transactions\")\n",
    "\n",
    "# SQL Query to count the number of accounts of each type\n",
    "account_type_count_df = spark.sql(\n",
    "    \"SELECT account_type, COUNT(DISTINCT account_id) as account_type_count \"\n",
    "    \"FROM transactions \"\n",
    "    \"GROUP BY account_type\"\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "account_type_count_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9cf6145-13d7-4523-b49c-51b5ab09326c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----------+\n",
      "|account_id|balance|latest_date|\n",
      "+----------+-------+-----------+\n",
      "|         3|  370.0| 2023-07-21|\n",
      "|         5|  200.0| 2023-07-30|\n",
      "|         1|  150.0| 2023-07-05|\n",
      "|         4|  800.0| 2023-07-25|\n",
      "|         2|  400.0| 2023-07-12|\n",
      "+----------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "transactions_df = spark.read.csv(\"transactions.csv\", header=True)\n",
    "\n",
    "# Convert amount to float and transaction_date to date type\n",
    "transactions_df = transactions_df.withColumn(\"amount\", F.col(\"amount\").cast(\"float\"))\n",
    "transactions_df = transactions_df.withColumn(\"transaction_date\", F.col(\"transaction_date\").cast(\"date\"))\n",
    "\n",
    "# Register the DataFrame as a SQL temporary view\n",
    "transactions_df.createOrReplaceTempView(\"transactions\")\n",
    "\n",
    "# SQL Query to calculate the balance and latest transaction date for each account\n",
    "result_df = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        account_id, \n",
    "        SUM(amount) as balance, \n",
    "        MAX(transaction_date) as latest_date\n",
    "    FROM transactions\n",
    "    GROUP BY account_id\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Convert balance to string type as per requirement\n",
    "result_df = result_df.withColumn(\"balance\", F.col(\"balance\").cast(\"string\"))\n",
    "\n",
    "# Show the result\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bc57b4a-cea6-456e-ac0f-cdec24e8140e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|full_name| 2023|\n",
      "+---------+-----+\n",
      "| John Doe|200.0|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "account_df = spark.read.csv(\"account.csv\", header=True)\n",
    "\n",
    "# Read transactions.csv file\n",
    "transactions_df = spark.read.csv(\"transactions.csv\", header=True)\n",
    "\n",
    "# Convert amount to float and transaction_date to date type\n",
    "transactions_df = transactions_df.withColumn(\"amount\", F.col(\"amount\").cast(\"float\"))\n",
    "transactions_df = transactions_df.withColumn(\"transaction_date\", F.col(\"transaction_date\").cast(\"date\"))\n",
    "\n",
    "# Extract year from transaction_date\n",
    "transactions_df = transactions_df.withColumn(\"year\", F.year(\"transaction_date\"))\n",
    "\n",
    "# Filter for Swiss users and join with transactions\n",
    "swiss_users_df = account_df.filter(account_df.country == \"USA\") # Assuming \"CHE\" stands for Switzerland\n",
    "joined_df = transactions_df.join(swiss_users_df, transactions_df.account_id == swiss_users_df.id)\n",
    "\n",
    "# Calculate user full name and filter for positive transactions (earnings)\n",
    "joined_df = joined_df.withColumn(\"full_name\", F.concat_ws(\" \", \"first_name\", \"last_name\"))\n",
    "positive_transactions_df = joined_df.filter(joined_df.amount > 0)\n",
    "\n",
    "# Calculate total earnings for each Swiss user by year\n",
    "pivot_df = positive_transactions_df.groupBy(\"full_name\").pivot(\"year\").sum(\"amount\")\n",
    "\n",
    "# Show the result\n",
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "476d8889-abd1-42e6-a8b2-0a1a6804ee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_transaction_level(spark, transactions_df):\n",
    "    # Convert amount to float if it's not already\n",
    "    transactions_df = transactions_df.withColumn(\"amount\", F.col(\"amount\").cast(\"float\"))\n",
    "\n",
    "    # Calculate 25th and 75th percentiles\n",
    "    quantiles = transactions_df.approxQuantile(\"amount\", [0.25, 0.75], 0.01)\n",
    "    lower_quantile = quantiles[0]\n",
    "    upper_quantile = quantiles[1]\n",
    "    \n",
    "    # Add \"level\" column based on \"amount\"\n",
    "    transactions_with_level = transactions_df.withColumn(\n",
    "        \"level\",\n",
    "        F.when(F.col(\"amount\") > upper_quantile, \"high\")\n",
    "        .when((F.col(\"amount\") > lower_quantile) & (F.col(\"amount\") <= upper_quantile), \"average\")\n",
    "        .otherwise(\"low\")\n",
    "    )\n",
    "\n",
    "    # Return the DataFrame with the additional \"level\" column\n",
    "    return transactions_with_level\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbb31723-879e-49c9-a58a-bf3d180f36cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------------+----------------+----+\n",
      "|account_id|amount|account_type|transaction_date|year|\n",
      "+----------+------+------------+----------------+----+\n",
      "|         1| 200.0|     savings|      2023-07-01|2023|\n",
      "|         1| -50.0|     savings|      2023-07-05|2023|\n",
      "|         2| 500.0|    checking|      2023-07-10|2023|\n",
      "|         2|-100.0|    checking|      2023-07-12|2023|\n",
      "|         3| 400.0|     savings|      2023-07-20|2023|\n",
      "|         3| -30.0|     savings|      2023-07-21|2023|\n",
      "|         4|1000.0|     savings|      2023-07-22|2023|\n",
      "|         4|-200.0|     savings|      2023-07-25|2023|\n",
      "|         5| 250.0|    checking|      2023-07-28|2023|\n",
      "|         5| -50.0|    checking|      2023-07-30|2023|\n",
      "+----------+------+------------+----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a99f06b-c6f4-4a66-a11f-5da6f4e6cb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_with_level = add_transaction_level(spark, transactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53380faf-e935-4c1f-8966-88fdfa835c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------------+----------------+----+-------+\n",
      "|account_id|amount|account_type|transaction_date|year|  level|\n",
      "+----------+------+------------+----------------+----+-------+\n",
      "|         1| 200.0|     savings|      2023-07-01|2023|average|\n",
      "|         1| -50.0|     savings|      2023-07-05|2023|    low|\n",
      "|         2| 500.0|    checking|      2023-07-10|2023|   high|\n",
      "|         2|-100.0|    checking|      2023-07-12|2023|    low|\n",
      "|         3| 400.0|     savings|      2023-07-20|2023|average|\n",
      "|         3| -30.0|     savings|      2023-07-21|2023|average|\n",
      "|         4|1000.0|     savings|      2023-07-22|2023|   high|\n",
      "|         4|-200.0|     savings|      2023-07-25|2023|    low|\n",
      "|         5| 250.0|    checking|      2023-07-28|2023|average|\n",
      "|         5| -50.0|    checking|      2023-07-30|2023|    low|\n",
      "+----------+------+------------+----------------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_with_level.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fec02808-dbe0-44b6-b8dc-f261bd8bfdf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  id                Col1   Col2\n",
      "0  1       one,two,three    one\n",
      "1  2       four,one,five    six\n",
      "2  3  seven,nine,one,two  eight\n",
      "3  4      two,three,five   five\n",
      "4  5        six,five,one  seven\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def read_pyspark_output_to_dataframe(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Remove the separator lines and strip whitespace\n",
    "    clean_lines = [line.strip() for line in lines if re.search(r'[a-zA-Z0-9]+', line)]\n",
    "    columns_line = clean_lines[0]\n",
    "    columns = columns_line.split(\"|\")[1:-1]\n",
    "    columns = [col.strip() for col in columns]\n",
    "\n",
    "    # print(columns)\n",
    "    rows = []\n",
    "    for line in clean_lines[1:]:\n",
    "        row_data = line.split(\"|\")[1:-1]\n",
    "        row_data = [item.strip() for item in row_data]\n",
    "        rows.append(row_data)\n",
    "    # print(rows)\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "file_path = 'parse_file.txt'\n",
    "df = read_pyspark_output_to_dataframe(file_path)\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
