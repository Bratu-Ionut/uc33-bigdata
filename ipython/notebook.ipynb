{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84a5a612-64ce-4e43-b5fa-90a674bbe5d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[713, 207, 522, 383, 266]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark  \n",
    "\n",
    "sc = pyspark.SparkContext('local[*]') \n",
    "\n",
    "# do something to prove it works \n",
    "\n",
    "rdd = sc.parallelize(range(1000)) \n",
    "\n",
    "rdd.takeSample(False, 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8786b25d-1d8b-4361-8e12-a362ba608e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "account.csv\t\t  notebook.ipynb  transactions.csv\n",
      "country_abbreviation.csv  parse_file.txt\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a8c6015-6840-4dcb-8942-7b77b6b3ebb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The account.csv file has 500000 rows.\n"
     ]
    }
   ],
   "source": [
    "account_df = spark.read.csv(\"../../data/accounts.csv\", header=True)\n",
    "account_count = account_df.count()\n",
    "print(f\"The account.csv file has {account_count} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f40e1b22-0baf-4cdf-8446-6cff9b5f408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSV File Count Example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8f22abf-58dd-46e5-bce4-00b7c83ce633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The account.csv file has 500000 rows.\n",
      "The transactions.csv file has 5000000 rows.\n",
      "The country_abbreviation.csv file has 121 rows.\n"
     ]
    }
   ],
   "source": [
    "# Read account.csv file\n",
    "account_df = spark.read.csv(\"../../data/accounts.csv\", header=True,sep=\";\")\n",
    "account_count = account_df.count()\n",
    "print(f\"The account.csv file has {account_count} rows.\")\n",
    "\n",
    "# Read transactions.csv file\n",
    "transactions_df = spark.read.csv(\"../../data/transactions.csv\", header=True,sep=\";\")\n",
    "transactions_count = transactions_df.count()\n",
    "print(f\"The transactions.csv file has {transactions_count} rows.\")\n",
    "\n",
    "# Read country_abbreviation.csv file\n",
    "country_abbreviation_df = spark.read.csv(\"../../data/country_abbreviation.csv\", header=True,sep=\";\")\n",
    "country_abbreviation_count = country_abbreviation_df.count()\n",
    "print(f\"The country_abbreviation.csv file has {country_abbreviation_count} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad2353a0-5ec0-4ad5-942e-7b9fcdf4d020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------------+----------------+-------+\n",
      "|    id| amount|account_type|transaction_date|country|\n",
      "+------+-------+------------+----------------+-------+\n",
      "|179528|-730.86|    Business|      2013-07-10|     SV|\n",
      "|378343|-946.98|    Personal|      2018-04-06|     YE|\n",
      "| 75450|7816.92|Professional|      2016-11-20|     SI|\n",
      "|357719| 704.02|    Business|      2016-11-06|     ID|\n",
      "|110511| 3462.6|    Personal|      2018-01-18|     BS|\n",
      "+------+-------+------------+----------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8de3f430-abc5-4487-b0ad-14092bc3de9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `account_id` cannot be resolved. Did you mean one of the following? [`transactions`.`id`, `transactions`.`country`, `transactions`.`amount`, `transactions`.`account_type`, `transactions`.`transaction_date`].; line 1 pos 36;\n'Aggregate [account_type#516], [account_type#516, 'COUNT(distinct 'account_id) AS account_type_count#524]\n+- SubqueryAlias transactions\n   +- View (`transactions`, [id#514,amount#515,account_type#516,transaction_date#517,country#518])\n      +- Relation [id#514,amount#515,account_type#516,transaction_date#517,country#518] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m transactions_df\u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransactions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# SQL Query to count the number of accounts of each type\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m account_type_count_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT account_type, COUNT(DISTINCT account_id) as account_type_count \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFROM transactions \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGROUP BY account_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Show the result\u001b[39;00m\n\u001b[1;32m     15\u001b[0m account_type_count_df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1439\u001b[0m     litArgs \u001b[38;5;241m=\u001b[39m {k: _to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `account_id` cannot be resolved. Did you mean one of the following? [`transactions`.`id`, `transactions`.`country`, `transactions`.`amount`, `transactions`.`account_type`, `transactions`.`transaction_date`].; line 1 pos 36;\n'Aggregate [account_type#516], [account_type#516, 'COUNT(distinct 'account_id) AS account_type_count#524]\n+- SubqueryAlias transactions\n   +- View (`transactions`, [id#514,amount#515,account_type#516,transaction_date#517,country#518])\n      +- Relation [id#514,amount#515,account_type#516,transaction_date#517,country#518] csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Spark SQL 2\n",
    "transactions_df = spark.read.csv(\"../../data/transactions.csv\", header=True,sep=\";\")\n",
    "\n",
    "# Register the DataFrame as a SQL temporary table\n",
    "transactions_df.createOrReplaceTempView(\"transactions\")\n",
    "\n",
    "# SQL Query to count the number of accounts of each type\n",
    "account_type_count_df = spark.sql(\n",
    "    \"SELECT account_type, COUNT(DISTINCT account_id) as account_type_count \"\n",
    "    \"FROM transactions \"\n",
    "    \"GROUP BY account_type\"\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "account_type_count_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9cf6145-13d7-4523-b49c-51b5ab09326c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----------+\n",
      "|account_id|balance|latest_date|\n",
      "+----------+-------+-----------+\n",
      "|         3|  370.0| 2023-07-21|\n",
      "|         5|  200.0| 2023-07-30|\n",
      "|         1|  150.0| 2023-07-05|\n",
      "|         4|  800.0| 2023-07-25|\n",
      "|         2|  400.0| 2023-07-12|\n",
      "+----------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "transactions_df = spark.read.csv(\"transactions.csv\", header=True)\n",
    "\n",
    "# Convert amount to float and transaction_date to date type\n",
    "transactions_df = transactions_df.withColumn(\"amount\", F.col(\"amount\").cast(\"float\"))\n",
    "transactions_df = transactions_df.withColumn(\"transaction_date\", F.col(\"transaction_date\").cast(\"date\"))\n",
    "\n",
    "# Register the DataFrame as a SQL temporary view\n",
    "transactions_df.createOrReplaceTempView(\"transactions\")\n",
    "\n",
    "# SQL Query to calculate the balance and latest transaction date for each account\n",
    "result_df = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        account_id, \n",
    "        SUM(amount) as balance, \n",
    "        MAX(transaction_date) as latest_date\n",
    "    FROM transactions\n",
    "    GROUP BY account_id\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Convert balance to string type as per requirement\n",
    "result_df = result_df.withColumn(\"balance\", F.col(\"balance\").cast(\"string\"))\n",
    "\n",
    "# Show the result\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bc57b4a-cea6-456e-ac0f-cdec24e8140e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|full_name| 2023|\n",
      "+---------+-----+\n",
      "| John Doe|200.0|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "account_df = spark.read.csv(\"account.csv\", header=True)\n",
    "\n",
    "# Read transactions.csv file\n",
    "transactions_df = spark.read.csv(\"transactions.csv\", header=True)\n",
    "\n",
    "# Convert amount to float and transaction_date to date type\n",
    "transactions_df = transactions_df.withColumn(\"amount\", F.col(\"amount\").cast(\"float\"))\n",
    "transactions_df = transactions_df.withColumn(\"transaction_date\", F.col(\"transaction_date\").cast(\"date\"))\n",
    "\n",
    "# Extract year from transaction_date\n",
    "transactions_df = transactions_df.withColumn(\"year\", F.year(\"transaction_date\"))\n",
    "\n",
    "# Filter for Swiss users and join with transactions\n",
    "swiss_users_df = account_df.filter(account_df.country == \"USA\") # Assuming \"CHE\" stands for Switzerland\n",
    "joined_df = transactions_df.join(swiss_users_df, transactions_df.account_id == swiss_users_df.id)\n",
    "\n",
    "# Calculate user full name and filter for positive transactions (earnings)\n",
    "joined_df = joined_df.withColumn(\"full_name\", F.concat_ws(\" \", \"first_name\", \"last_name\"))\n",
    "positive_transactions_df = joined_df.filter(joined_df.amount > 0)\n",
    "\n",
    "# Calculate total earnings for each Swiss user by year\n",
    "pivot_df = positive_transactions_df.groupBy(\"full_name\").pivot(\"year\").sum(\"amount\")\n",
    "\n",
    "# Show the result\n",
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "476d8889-abd1-42e6-a8b2-0a1a6804ee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_transaction_level(spark, transactions_df):\n",
    "    # Convert amount to float if it's not already\n",
    "    transactions_df = transactions_df.withColumn(\"amount\", F.col(\"amount\").cast(\"float\"))\n",
    "\n",
    "    # Calculate 25th and 75th percentiles\n",
    "    quantiles = transactions_df.approxQuantile(\"amount\", [0.25, 0.75], 0.01)\n",
    "    lower_quantile = quantiles[0]\n",
    "    upper_quantile = quantiles[1]\n",
    "    \n",
    "    # Add \"level\" column based on \"amount\"\n",
    "    transactions_with_level = transactions_df.withColumn(\n",
    "        \"level\",\n",
    "        F.when(F.col(\"amount\") > upper_quantile, \"high\")\n",
    "        .when((F.col(\"amount\") > lower_quantile) & (F.col(\"amount\") <= upper_quantile), \"average\")\n",
    "        .otherwise(\"low\")\n",
    "    )\n",
    "\n",
    "    # Return the DataFrame with the additional \"level\" column\n",
    "    return transactions_with_level\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbb31723-879e-49c9-a58a-bf3d180f36cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------------+----------------+----+\n",
      "|account_id|amount|account_type|transaction_date|year|\n",
      "+----------+------+------------+----------------+----+\n",
      "|         1| 200.0|     savings|      2023-07-01|2023|\n",
      "|         1| -50.0|     savings|      2023-07-05|2023|\n",
      "|         2| 500.0|    checking|      2023-07-10|2023|\n",
      "|         2|-100.0|    checking|      2023-07-12|2023|\n",
      "|         3| 400.0|     savings|      2023-07-20|2023|\n",
      "|         3| -30.0|     savings|      2023-07-21|2023|\n",
      "|         4|1000.0|     savings|      2023-07-22|2023|\n",
      "|         4|-200.0|     savings|      2023-07-25|2023|\n",
      "|         5| 250.0|    checking|      2023-07-28|2023|\n",
      "|         5| -50.0|    checking|      2023-07-30|2023|\n",
      "+----------+------+------------+----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a99f06b-c6f4-4a66-a11f-5da6f4e6cb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_with_level = add_transaction_level(spark, transactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53380faf-e935-4c1f-8966-88fdfa835c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------------+----------------+----+-------+\n",
      "|account_id|amount|account_type|transaction_date|year|  level|\n",
      "+----------+------+------------+----------------+----+-------+\n",
      "|         1| 200.0|     savings|      2023-07-01|2023|average|\n",
      "|         1| -50.0|     savings|      2023-07-05|2023|    low|\n",
      "|         2| 500.0|    checking|      2023-07-10|2023|   high|\n",
      "|         2|-100.0|    checking|      2023-07-12|2023|    low|\n",
      "|         3| 400.0|     savings|      2023-07-20|2023|average|\n",
      "|         3| -30.0|     savings|      2023-07-21|2023|average|\n",
      "|         4|1000.0|     savings|      2023-07-22|2023|   high|\n",
      "|         4|-200.0|     savings|      2023-07-25|2023|    low|\n",
      "|         5| 250.0|    checking|      2023-07-28|2023|average|\n",
      "|         5| -50.0|    checking|      2023-07-30|2023|    low|\n",
      "+----------+------+------------+----------------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_with_level.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fec02808-dbe0-44b6-b8dc-f261bd8bfdf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  id                Col1   Col2\n",
      "0  1       one,two,three    one\n",
      "1  2       four,one,five    six\n",
      "2  3  seven,nine,one,two  eight\n",
      "3  4      two,three,five   five\n",
      "4  5        six,five,one  seven\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def read_pyspark_output_to_dataframe(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Remove the separator lines and strip whitespace\n",
    "    clean_lines = [line.strip() for line in lines if re.search(r'[a-zA-Z0-9]+', line)]\n",
    "    columns_line = clean_lines[0]\n",
    "    columns = columns_line.split(\"|\")[1:-1]\n",
    "    columns = [col.strip() for col in columns]\n",
    "\n",
    "    # print(columns)\n",
    "    rows = []\n",
    "    for line in clean_lines[1:]:\n",
    "        row_data = line.split(\"|\")[1:-1]\n",
    "        row_data = [item.strip() for item in row_data]\n",
    "        rows.append(row_data)\n",
    "    # print(rows)\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "file_path = 'parse_file.txt'\n",
    "df = read_pyspark_output_to_dataframe(file_path)\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
