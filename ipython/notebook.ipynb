{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84a5a612-64ce-4e43-b5fa-90a674bbe5d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[713, 207, 522, 383, 266]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark  \n",
    "\n",
    "sc = pyspark.SparkContext('local[*]') \n",
    "\n",
    "# do something to prove it works \n",
    "\n",
    "rdd = sc.parallelize(range(1000)) \n",
    "\n",
    "rdd.takeSample(False, 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8786b25d-1d8b-4361-8e12-a362ba608e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "account.csv\t\t  notebook.ipynb  transactions.csv\n",
      "country_abbreviation.csv  parse_file.txt\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a8c6015-6840-4dcb-8942-7b77b6b3ebb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The account.csv file has 500000 rows.\n"
     ]
    }
   ],
   "source": [
    "account_df = spark.read.csv(\"../../data/accounts.csv\", header=True)\n",
    "account_count = account_df.count()\n",
    "print(f\"The account.csv file has {account_count} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f40e1b22-0baf-4cdf-8446-6cff9b5f408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSV File Count Example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8f22abf-58dd-46e5-bce4-00b7c83ce633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The account.csv file has 500000 rows.\n",
      "The transactions.csv file has 5000000 rows.\n",
      "The country_abbreviation.csv file has 121 rows.\n"
     ]
    }
   ],
   "source": [
    "# Read account.csv file\n",
    "account_df = spark.read.csv(\"../../data/accounts.csv\", header=True,sep=\";\")\n",
    "account_count = account_df.count()\n",
    "print(f\"The account.csv file has {account_count} rows.\")\n",
    "\n",
    "# Read transactions.csv file\n",
    "transactions_df = spark.read.csv(\"../../data/transactions.csv\", header=True,sep=\";\")\n",
    "transactions_count = transactions_df.count()\n",
    "print(f\"The transactions.csv file has {transactions_count} rows.\")\n",
    "\n",
    "# Read country_abbreviation.csv file\n",
    "country_abbreviation_df = spark.read.csv(\"../../data/country_abbreviation.csv\", header=True,sep=\";\")\n",
    "country_abbreviation_count = country_abbreviation_df.count()\n",
    "print(f\"The country_abbreviation.csv file has {country_abbreviation_count} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad2353a0-5ec0-4ad5-942e-7b9fcdf4d020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------------+----------------+-------+\n",
      "|    id| amount|account_type|transaction_date|country|\n",
      "+------+-------+------------+----------------+-------+\n",
      "|179528|-730.86|    Business|      2013-07-10|     SV|\n",
      "|378343|-946.98|    Personal|      2018-04-06|     YE|\n",
      "| 75450|7816.92|Professional|      2016-11-20|     SI|\n",
      "|357719| 704.02|    Business|      2016-11-06|     ID|\n",
      "|110511| 3462.6|    Personal|      2018-01-18|     BS|\n",
      "+------+-------+------------+----------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8de3f430-abc5-4487-b0ad-14092bc3de9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|account_type|account_type_count|\n",
      "+------------+------------------+\n",
      "|    Personal|            481997|\n",
      "|Professional|            482170|\n",
      "|    Business|            482350|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Spark SQL 2\n",
    "transactions_df = spark.read.csv(\"../../data/transactions.csv\", header=True,sep=\";\")\n",
    "\n",
    "# Register the DataFrame as a SQL temporary table\n",
    "transactions_df.createOrReplaceTempView(\"transactions\")\n",
    "\n",
    "# SQL Query to count the number of accounts of each type\n",
    "account_type_count_df = spark.sql(\n",
    "    \"SELECT account_type, COUNT(DISTINCT id) as account_type_count \"\n",
    "    \"FROM transactions \"\n",
    "    \"GROUP BY account_type\"\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "account_type_count_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9cf6145-13d7-4523-b49c-51b5ab09326c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+-----------+\n",
      "|    id|           balance|latest_date|\n",
      "+------+------------------+-----------+\n",
      "| 68325|53232.409576416016| 2021-12-02|\n",
      "|249223|30680.630126953125| 2020-04-13|\n",
      "|113602|     54096.0703125| 2021-12-27|\n",
      "|448517|52082.130615234375| 2021-02-04|\n",
      "|193630| 63034.56997680664| 2021-02-05|\n",
      "|417334|21550.009552001953| 2020-11-30|\n",
      "|351084|  35345.1498708725| 2021-10-21|\n",
      "|429742| 51014.89072418213| 2021-12-26|\n",
      "|195983|18586.459869384766| 2021-08-27|\n",
      "|454795| 68893.17016601562| 2021-02-21|\n",
      "|444212| 40396.46002960205| 2020-09-08|\n",
      "|414329| 45952.11022949219| 2019-04-26|\n",
      "|191501| 73625.31985473633| 2021-10-09|\n",
      "|376767|44612.060302734375| 2021-08-12|\n",
      "|355412|29777.969772338867| 2021-07-04|\n",
      "| 55456|48036.110580444336| 2019-10-15|\n",
      "|383353|43718.770080566406| 2021-10-25|\n",
      "|224178| 69300.92985534668| 2021-10-10|\n",
      "|462160| 37512.43994140625| 2021-02-15|\n",
      "|191369| 38177.33986663818| 2021-02-26|\n",
      "+------+------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "# Convert amount to float and transaction_date to date type\n",
    "transactions_df = transactions_df.withColumn(\"amount\", F.col(\"amount\").cast(\"float\"))\n",
    "transactions_df = transactions_df.withColumn(\"transaction_date\", F.col(\"transaction_date\").cast(\"date\"))\n",
    "\n",
    "# Register the DataFrame as a SQL temporary view\n",
    "transactions_df.createOrReplaceTempView(\"transactions\")\n",
    "\n",
    "# SQL Query to calculate the balance and latest transaction date for each account\n",
    "result_df = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        id, \n",
    "        SUM(amount) as balance, \n",
    "        MAX(transaction_date) as latest_date\n",
    "    FROM transactions\n",
    "    GROUP BY id\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Convert balance to string type as per requirement\n",
    "result_df = result_df.withColumn(\"balance\", F.col(\"balance\").cast(\"string\"))\n",
    "\n",
    "# Show the result\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "194fddf9-650f-45b2-8941-45fd8b3e86a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_df = spark.read.csv(\"../../data/accounts.csv\", header=True,sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e486c606-5a93-411a-ad91-1792f2a1d1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+---+-------+\n",
      "| id|first_name|last_name|age|country|\n",
      "+---+----------+---------+---+-------+\n",
      "|  1|     Darcy| Phillips| 24|     YE|\n",
      "|  2|    Amelia|   Wright| 66|     CN|\n",
      "|  3|     Haris|    Ellis| 61|     CR|\n",
      "|  4|      Tony|     Hall| 51|     JO|\n",
      "|  5|     Rubie|  Stewart| 27|     RO|\n",
      "+---+----------+---------+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "account_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2bc57b4a-cea6-456e-ac0f-cdec24e8140e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+-----------------+----------------+-----------------+------------------+------------------+----------------+-----------------+-----------------+------------------+--------------+\n",
      "|      full_name|              2011|             2012|            2013|             2014|              2015|              2016|            2017|             2018|             2019|              2020|          2021|\n",
      "+---------------+------------------+-----------------+----------------+-----------------+------------------+------------------+----------------+-----------------+-----------------+------------------+--------------+\n",
      "|  Connie Gibson|              null|    8486.83984375|3664.68994140625| 7385.68994140625|              null|  1717.47998046875|2436.81005859375|             null|   5545.759765625|              null|          null|\n",
      "|  Justin Cooper|20224.619873046875| 5977.14990234375|            null|16800.74951171875|  6475.06982421875|              null|            null|   2582.080078125|   5888.009765625|1988.9000244140625|          null|\n",
      "|Kirsten Stevens| 1008.239990234375|             null|366.760009765625|   8655.740234375|              null|1080.0799560546875|            null|             null|             null|   9788.5498046875|          null|\n",
      "|    Amber Evans|              null|3798.159912109375|            null|             null|              null|13363.730224609375|  7020.419921875|6696.239990234375|942.8200073242188| 11777.36962890625|          null|\n",
      "|  Arnold Morris|              null|             null|            null|12931.31997680664|2026.0699462890625|              null|2641.35009765625|             null|3427.389892578125|              null|8965.330078125|\n",
      "+---------------+------------------+-----------------+----------------+-----------------+------------------+------------------+----------------+-----------------+-----------------+------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert amount to float and transaction_date to date type\n",
    "transactions_df = transactions_df.withColumn(\"amount\", F.col(\"amount\").cast(\"float\"))\n",
    "transactions_df = transactions_df.withColumn(\"transaction_date\", F.col(\"transaction_date\").cast(\"date\"))\n",
    "\n",
    "# Extract year from transaction_date\n",
    "transactions_df = transactions_df.withColumn(\"year\", F.year(\"transaction_date\"))\n",
    "\n",
    "# Filter for Swiss users and join with transactions\n",
    "swiss_users_df = account_df.filter(account_df.country == \"CH\") # Assuming \"CH\" stands for Switzerland\n",
    "joined_df = transactions_df.join(swiss_users_df, transactions_df.id == swiss_users_df.id)\n",
    "\n",
    "# Calculate user full name and filter for positive transactions (earnings)\n",
    "joined_df = joined_df.withColumn(\"full_name\", F.concat_ws(\" \", \"first_name\", \"last_name\"))\n",
    "positive_transactions_df = joined_df.filter(joined_df.amount > 0)\n",
    "\n",
    "# Calculate total earnings for each Swiss user by year\n",
    "pivot_df = positive_transactions_df.groupBy(\"full_name\").pivot(\"year\").sum(\"amount\")\n",
    "\n",
    "# Show the result\n",
    "pivot_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "13056487-f938-47ae-92b3-136c6269d59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "def add_transaction_level(spark, transactions_df):\n",
    "    # Convert amount to float if it's not already\n",
    "    transactions_df = transactions_df.withColumn(\"amount\", col(\"amount\").cast(\"float\"))\n",
    "\n",
    "    # Use SQL expressions to calculate quantiles directly\n",
    "    transactions_with_level = transactions_df.withColumn(\n",
    "        \"level\",\n",
    "        expr(\n",
    "            \"CASE WHEN amount > percentile_approx(amount, 0.75) OVER () THEN 'high' \"\n",
    "            \"WHEN amount > percentile_approx(amount, 0.25) OVER () AND amount <= percentile_approx(amount, 0.75) OVER () THEN 'average' \"\n",
    "            \"ELSE 'low' END\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Return the DataFrame with the additional \"level\" column\n",
    "    return transactions_with_level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a99f06b-c6f4-4a66-a11f-5da6f4e6cb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_with_level = add_transaction_level(spark, transactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "53380faf-e935-4c1f-8966-88fdfa835c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------------+----------------+-------+----+-------+\n",
      "|    id| amount|account_type|transaction_date|country|year|  level|\n",
      "+------+-------+------------+----------------+-------+----+-------+\n",
      "|179528|-730.86|    Business|      2013-07-10|     SV|2013|    low|\n",
      "|378343|-946.98|    Personal|      2018-04-06|     YE|2018|    low|\n",
      "| 75450|7816.92|Professional|      2016-11-20|     SI|2016|   high|\n",
      "|357719| 704.02|    Business|      2016-11-06|     ID|2016|    low|\n",
      "|110511| 3462.6|    Personal|      2018-01-18|     BS|2018|average|\n",
      "|461830| 762.81|Professional|      2017-06-20|     CN|2017|    low|\n",
      "| 30180|5390.24|Professional|      2021-05-26|     GN|2021|average|\n",
      "| 65398|4765.77|    Personal|      2018-05-01|     TR|2018|average|\n",
      "|170899|8775.89|    Business|      2013-10-16|     SK|2013|   high|\n",
      "|234300|8455.18|Professional|      2015-10-06|     LU|2015|   high|\n",
      "|208027| 6244.1|    Business|      2020-03-06|     AE|2020|average|\n",
      "|161212|5904.56|    Personal|      2016-09-07|     EG|2016|average|\n",
      "|105372|4079.76|Professional|      2015-02-12|     MT|2015|average|\n",
      "|205321| 3570.4|Professional|      2012-07-02|     MU|2012|average|\n",
      "|410863|2328.83|    Business|      2012-12-20|     SR|2012|average|\n",
      "|486752| 5454.8|Professional|      2015-02-10|     CU|2015|average|\n",
      "|208564|8695.17|    Personal|      2013-01-03|     IT|2013|   high|\n",
      "|196682|-905.87|    Personal|      2019-01-28|     HU|2019|    low|\n",
      "|491196|8781.02|Professional|      2017-05-11|     IR|2017|   high|\n",
      "|108286|3485.95|    Personal|      2011-12-13|     ZW|2011|average|\n",
      "+------+-------+------------+----------------+-------+----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_with_level.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dd7320e7-3712-422a-b94d-df785b5821f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+\n",
      "| id|Col1|Col2|\n",
      "+---+----+----+\n",
      "|   |   i|   d|\n",
      "|   |    |   1|\n",
      "|   |    |   2|\n",
      "|   |    |   3|\n",
      "|   |    |   4|\n",
      "+---+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = 'parse_file.txt'\n",
    "df = read_pyspark_output_to_dataframe(spark, file_path)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "76aaced5-3c5b-440b-8a79-5ea3f76d6334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def read_pyspark_output_to_dataframe(spark, file_path):\n",
    "    # Read the text file as a DataFrame\n",
    "    df = spark.read.text(file_path)\n",
    "\n",
    "    # Define a regular expression pattern to match rows with alphanumeric characters\n",
    "    pattern = r'[a-zA-Z0-9]+'\n",
    "\n",
    "    # Filter the rows based on the pattern\n",
    "    filtered_df = df.filter(col(\"value\").rlike(pattern))\n",
    "\n",
    "    # Extract columns from the first row\n",
    "    columns = filtered_df.first().value.split(\"|\")[1:-1]\n",
    "    columns = [col.strip() for col in columns]\n",
    "\n",
    "    # Split and clean the data for rows\n",
    "    cleaned_df = filtered_df.selectExpr(\n",
    "        *[f\"split(value, '\\\\|')[{i+1}] as {col}\" for i, col in enumerate(columns)]\n",
    "    )\n",
    "\n",
    "    # Return the DataFrame\n",
    "    return cleaned_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fec02808-dbe0-44b6-b8dc-f261bd8bfdf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  id                Col1   Col2\n",
      "0  1       one,two,three    one\n",
      "1  2       four,one,five    six\n",
      "2  3  seven,nine,one,two  eight\n",
      "3  4      two,three,five   five\n",
      "4  5        six,five,one  seven\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def read_pyspark_output_to_dataframe(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Remove the separator lines and strip whitespace\n",
    "    clean_lines = [line.strip() for line in lines if re.search(r'[a-zA-Z0-9]+', line)]\n",
    "    columns_line = clean_lines[0]\n",
    "    columns = columns_line.split(\"|\")[1:-1]\n",
    "    columns = [col.strip() for col in columns]\n",
    "\n",
    "    # print(columns)\n",
    "    rows = []\n",
    "    for line in clean_lines[1:]:\n",
    "        row_data = line.split(\"|\")[1:-1]\n",
    "        row_data = [item.strip() for item in row_data]\n",
    "        rows.append(row_data)\n",
    "    # print(rows)\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "file_path = 'parse_file.txt'\n",
    "df = read_pyspark_output_to_dataframe(file_path)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2263d9d7-5125-4827-a23c-ba0fabb42478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|| id|            ...|\n",
      "||  1|     one,two...|\n",
      "||  2|     four,on...|\n",
      "||  3|seven,nine,o...|\n",
      "||  4|    two,thre...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_pyspark_output_to_dataframe(spark, file_path):\n",
    "    # Read the text file as a DataFrame\n",
    "    df = spark.read.text(file_path)\n",
    "\n",
    "    # Define a regular expression pattern to match rows with alphanumeric characters\n",
    "    pattern = r'[a-zA-Z0-9]+'\n",
    "\n",
    "    # Filter the rows based on the pattern and remove separator lines\n",
    "    filtered_df = df.filter(col(\"value\").rlike(pattern) & (col(\"value\").like(\"%|%\")))\n",
    "\n",
    "    # Extract columns from the first row\n",
    "    columns_line = filtered_df.first().value\n",
    "    columns = columns_line.split(\"|\")[1:-1]\n",
    "    columns = [col.strip() for col in columns]\n",
    "\n",
    "    # Return the DataFrame\n",
    "    return columns,filtered_df\n",
    "\n",
    "file_path = 'parse_file.txt'\n",
    "columns,df = read_pyspark_output_to_dataframe(spark, file_path)\n",
    "df.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "35f47dc7-3ee3-43d8-80e0-8d1be72bd7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "def read_pyspark_output_to_dataframe(spark, file_path):\n",
    "    # Read the text file as a DataFrame\n",
    "    df = spark.read.text(file_path)\n",
    "\n",
    "    # Define a regular expression pattern to match rows with alphanumeric characters\n",
    "    pattern = r'[a-zA-Z0-9]+'\n",
    "\n",
    "    # Filter the rows based on the pattern and remove separator lines\n",
    "    filtered_df = df.filter(col(\"value\").rlike(pattern) & (col(\"value\").like(\"%|%\")))\n",
    "\n",
    "    # Extract column names from the first row (skip the first line)\n",
    "    first_row = filtered_df.first()\n",
    "    columns = [col.strip() for col in first_row.value.split(\"|\")[1:-1]]\n",
    "\n",
    "    # Remove the first line (header)\n",
    "    data_df = filtered_df.dropDuplicates(subset=[\"value\"])\n",
    "\n",
    "    # Split the remaining lines into columns based on \"|\"\n",
    "    for i, col_name in enumerate(columns):\n",
    "        data_df = data_df.withColumn(col_name, split(data_df[\"value\"], \"\\\\|\")[i + 1])\n",
    "\n",
    "    # Drop the original \"value\" column\n",
    "    data_df = data_df.drop(\"value\")\n",
    "\n",
    "    # Return the DataFrame\n",
    "    return data_df\n",
    "\n",
    "# Create a Spark session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fc77c946-575c-4e1a-8808-7891f9c4b22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'parse_file.txt'\n",
    "df = read_pyspark_output_to_dataframe(spark, file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ac057244-ea92-4c4c-b6f9-3d9304319d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----+\n",
      "| id|              Col1| Col2|\n",
      "+---+------------------+-----+\n",
      "| id|              Col1| Col2|\n",
      "|  5|      six,five,one|seven|\n",
      "|  2|     four,one,five|  six|\n",
      "|  1|     one,two,three|  one|\n",
      "|  3|seven,nine,one,two|eight|\n",
      "+---+------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0774ca-caf6-4e19-b68f-27d18b9dad37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
